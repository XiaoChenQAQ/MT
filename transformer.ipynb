{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "嵌入层和位置编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmdAndPos(nn.Module):\n",
    "    '''\n",
    "    处理好的句子序列,并给他加上position编码\n",
    "    参数(emb_size=d_model, seq_len, dict_number, padding_idx)\n",
    "    输入:(batch_size, seq_len)\n",
    "    输出:(batch_size, seq_len, emb_size)\n",
    "    test:\n",
    "    input1 = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n",
    "    net = nn.Sequential(EmdAndPos(16,4,10))\n",
    "    print(net(input1))\n",
    "    '''\n",
    "    def __init__(self,emb_size,seq_len,dict_number):\n",
    "        super(EmdAndPos, self).__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings=dict_number, embedding_dim=emb_size, padding_idx=0)\n",
    "        self.pos = self._position(emb_size,seq_len)\n",
    "\n",
    "    def _position(self,emb_size,seq_len):\n",
    "        '''\n",
    "        pos只与位置有关,没有学习过程,句子中的每一个单词产生一个描述位置的与词嵌入等长的向量,整个句子产生一个(seq_len,emb_size)的矩阵\n",
    "        输入:处理好的句子序列\n",
    "        输出:输出(seq_len , emb_size)的矩阵\n",
    "        '''\n",
    "        PE = np.zeros((seq_len,emb_size))\n",
    "        def func(pos,i,emb_size):\n",
    "            if i%2 ==0:\n",
    "                return math.sin(pos / (10000**(1.0*i/emb_size)))\n",
    "            else:\n",
    "                return math.cos(pos / (10000**(1.0*(i-1)/emb_size)))\n",
    "        for xy,val in np.ndenumerate(PE):\n",
    "            PE[xy]= func(xy[0], xy[1], emb_size)\n",
    "        pos_matrix = torch.from_numpy(PE)\n",
    "        return pos_matrix\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        X = self.emb(inputs)\n",
    "        # print(self.pos)\n",
    "        # print(\"*\"*80)\n",
    "        # print(X)\n",
    "        return X+self.pos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' \n",
    "    基于点乘的多头注意力层;\n",
    "    Q的维度(L,d_k),V的维度(L,d_k),V的维度(L,d_v);d_k,d_v分别表示key和value的大小,通常设置d_k=d_v=d_model\n",
    "    输入:(batch_size, seq_len, d_model)\n",
    "    输出:(batch_size, seq_len, d_model)\n",
    "    '''\n",
    "    def __init__(self, seq_len,heads, d_model, d_k=None, d_v=None, dropout=0.1,decode=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_k = d_model if not d_k else d_k\n",
    "        self.d_v = d_model if not d_v else d_v\n",
    "        self.heads = heads\n",
    "        self.head_dim = d_model // heads\n",
    "        assert self.head_dim * heads == d_model ,\"heads必须能整除d_model\"\n",
    "        self.Q = nn.Linear(d_model,d_k, bias=False)\n",
    "        self.K = nn.Linear(d_model,d_k, bias=False)\n",
    "        self.V = nn.Linear(d_model,d_v, bias=False)\n",
    "\n",
    "        self.W_Q = nn.Parameter(data=torch.tensor(heads, d_model, d_k//heads),requires_grad=True)\n",
    "        self.W_K = nn.Parameter(data=torch.tensor(heads, d_model, d_k//heads),requires_grad=True)\n",
    "        self.W_V = nn.Parameter(data=torch.tensor(heads, d_model, d_v//heads),requires_grad=True)\n",
    "        \n",
    "        self.outputlinear = nn.Linear(d_k,d_model)\n",
    "        #self.W_O = nn.Parameter(data=torch.tensor())\n",
    "        if not decode:\n",
    "            self.mask = None\n",
    "        else:\n",
    "            self.mask = torch.mask_fill(self._make_mask(seq_len), value=float(\"-inf\"))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "            \n",
    "\n",
    "    def _make_mask(self, dim):\n",
    "        matirx = np.ones((dim, dim))\n",
    "        mask = torch.Tensor(np.tril(matirx))\n",
    "        return mask==1\n",
    "    \n",
    "    def _dotmulAtt(self, q, k, v):\n",
    "        '''\n",
    "        q,k,v向量点乘注意力\n",
    "        q,k,v输入维度(batch_size,seq_len,head_dim)\n",
    "        返回维度:(batch_size,seq_len, head_dim)\n",
    "        '''\n",
    "        return torch.bmm(self.softmax(torch.bmm(q,k.permute(0,2,1)) / math.sqrt(self.d_k) +self.mask),v)\n",
    "\n",
    "    \n",
    "    def forward(self, source, target, ):\n",
    "        # 将XY仿射变换成QKV\n",
    "        Q = self.Q(source)\n",
    "        K = self.K(target)\n",
    "        V = self.V(target)\n",
    "        #多头点积注意力\n",
    "        headlist = []\n",
    "        for i in range(self.heads):\n",
    "            head_i = self._dotmulAtt(Q @ self.W_Q[i], K @ self.W_K[i], V @ self.W_V[i])\n",
    "            headlist.append(head_i)\n",
    "        #连接后矩阵大小是(batch_size,seq_len,head_dim*heads)\n",
    "        output = self.outputlinear(torch.cat(headlist))\n",
    "        #经过线性层输出大小(batch_size,seq_len, d_model)\n",
    "        return output\n",
    "\n",
    "# class dotProdAttention(nn.Module):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#残差网络和层标准化\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, dropout=0.1, pre_norm = True):\n",
    "        super(AddNorm, self).__init__()\n",
    "        #self.dropout = nn.Dropout(dropout)\n",
    "        self.pre_norm = pre_norm\n",
    "        \n",
    "\n",
    "    def forward(self,x, sub_layer,**kwargs):\n",
    "        if self.pre_norm:\n",
    "            layer_norm = nn.LayerNorm(x.size()[1:])\n",
    "            out = layer_norm(x)\n",
    "            sub_output = sub_layer(out,**kwargs)\n",
    "            out = self.dropout(sub_output)\n",
    "            return out + x\n",
    "        else:\n",
    "            sub_output = sub_layer(x,**kwargs)\n",
    "            x = self.dropout(x + sub_output)\n",
    "            layer_norm = nn.LayerNorm(x.size()[1:])\n",
    "            out = layer_norm(x)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feed_Forward(nn.Module):\n",
    "    def __init__(self,d_model,hidden_dim=2048):\n",
    "        super(Feed_Forward, self).__init__()\n",
    "        self.L1 = nn.Linear(d_model,hidden_dim)\n",
    "        self.L2 = nn.Linear(hidden_dim,d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = self.relu(self.L1(x))\n",
    "        output = self.L2(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#编码器结构\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.positional_encoding = EmbAndPos(config.d_model)\n",
    "        self.muti_atten = MutiHeadAttention(config.d_model,config.dim_k,config.dim_v,config.n_heads)\n",
    "        self.feed_forward = Feed_Forward(config.d_model)\n",
    "\n",
    "        self.add_norm = Add_Norm()\n",
    "\n",
    "\n",
    "    def forward(self,x): # batch_size * seq_len 并且 x 的类型不是tensor，是普通list\n",
    "\n",
    "        x += self.positional_encoding(x.shape[1],config.d_model)\n",
    "        # print(\"After positional_encoding: {}\".format(x.size()))\n",
    "        output = self.add_norm(x,self.muti_atten,y=x)\n",
    "        output = self.add_norm(output,self.feed_forward)\n",
    "\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c65aff122350cd56b737f3106f4acefc74f496c18fafa7c35e00359104e37a88"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('dlpy': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
